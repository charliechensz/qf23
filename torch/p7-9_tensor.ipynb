{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的创建、和np的转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 tensor， 定义类型 \n",
    "x1 = torch.rand(2, 3)       # 0-1之间的随机数\n",
    "x2 = torch.randn(3, 4)      # 动态分布随机数\n",
    "x3 = torch.zeros(2, 3)\n",
    "x4 = torch.ones((2, 3, 4), dtype=torch.int64)   # 定义1数据的类型，预设是float\n",
    "print(x3) \n",
    "print(x4)\n",
    "\n",
    "# 显示tensor信息\n",
    "print(x4.size())       \n",
    "print(x4.shape)        # 类似size(), 但没有（）\n",
    "x4.size(1)\n",
    "x4.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换 list 到tensor; 定义和更改数据类型\n",
    "x = torch.tensor([6.1, 2])                          # 转换list，类型不变\n",
    "print(x)\n",
    "x = torch.tensor([6, 2], dtype=torch.float32)       # 制定转换成 float32类型\n",
    "print(x)\n",
    "\n",
    "print(x.type)\n",
    "print(x.type( torch.int64 ))                        # tensor再转换到int64\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np类型、tensor类型之间的转换 \n",
    "x1np = np.random.randn(2, 3)        # np创建一个 array, 随机分布\n",
    "print(x1np)\n",
    "x1 = torch.from_numpy( x1np )       # np 转换到 tensor\n",
    "print(x1)\n",
    "print(x1.numpy())                   # tensor再转回到np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量运算：相加、相减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 张量运算：相加、相减; \n",
    "print(x1)\n",
    "x2 = torch.rand(2,3)\n",
    "print(x2)\n",
    "x = x1 + x2\n",
    "print(x)\n",
    "print(x1 + 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 相加后是否赋值x1\n",
    "print(x1.add(x2))       # x1，不改变x1\n",
    "print(x1.add_(-x2))      # x1改变了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 变形\n",
    "print( x1.view(3,2) )\n",
    "# -1 ... 用法\n",
    "print( x1.view(-1, 1))      # n行，1列\n",
    "print( x1.view(1, -1))      # \n",
    "print( x1.view(2, -1))      # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均、求和、最大、最小\n",
    "print(x1.mean())\n",
    "print(x1.sum())\n",
    "print(x1.max())\n",
    "print(x1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x1.min().item() + 1)      # 单一的tensor变成数值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "张量的自动微分\n",
    "- requires_grad = True, 则在完成计算后，可以调用 .backword()并自动计算所有梯度。\n",
    "- tensor3个属性：data，grad，grad_fn\n",
    "  -  x = torch.ones(2,2, requires_grad=True)\n",
    "  -  x.data ，x的内容\n",
    "  -  x.grad , x的梯度\n",
    "- 改变\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)     # True 表示开始跟踪x的变化，计算梯度\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.data)\n",
    "print(x.grad)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "True\n",
      "None\n",
      "<AddBackward0 object at 0x000001A7ED54D570>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\temp\\ipykernel_17784\\1379503892.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y.data)\n",
    "print(y.requires_grad)  # x 的 grad属性传递到了 y\n",
    "print(y.grad)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n",
      "True\n",
      "None\n",
      "<AddBackward0 object at 0x000001A7EEA646A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\temp\\ipykernel_17784\\1776470700.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(z.grad)\n"
     ]
    }
   ],
   "source": [
    "z = y*y + 3\n",
    "print(z.data)\n",
    "print(z.requires_grad)  # x 的 grad属性传递到了 y\n",
    "print(z.grad)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stkdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
