{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.1', '0.15.2', 'cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from torch import nn\n",
    "import torchvision  # 图片、视频处理\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "torch.__version__, torchvision.__version__, device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToTensor: \n",
    "- 1、输入转为Tensor，\n",
    "- 2、图片格式为 channel, height, width; \n",
    "- 3、像素取值范围规范到0/1\n",
    "\n",
    "torch.utils.data.DataLoader, 作用：\n",
    "- 乱序， shuffle 默认为 True\n",
    "- 将数据采样为小批次， batch_size. batch太小会导致loss的剧烈震荡，太大则内存放不下，也会跨度过大，失去准确性；\n",
    "- num_workers, 子进程设置，更多进程参与\n",
    "- 设置批次处理函数 collate_fn, 用在文本等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.MNIST('data', \n",
    "                                      train=True, \n",
    "                                      download=True, \n",
    "                                      transform=ToTensor())\n",
    "test_ds = torchvision.datasets.MNIST('data', \n",
    "                                     train=False , \n",
    "                                     download=True, \n",
    "                                     transform=ToTensor())\n",
    "# test_ds = test_ds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)  # batch_size开始为64,可能是最佳，或128\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 28, 28]), torch.Size([2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(iter(train_dl))   # train dataloder 可以分解出X和Ydata\n",
    "imgs.shape, labels.shape   # 1: 黑白图片， 28x28的图片分辨率， 64张\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x100 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# labels\n",
    "plt.figure(figsize=(10, 1))\n",
    "for i, img in enumerate( imgs[:5] ): #取前10张图片   \n",
    "    print( img.shape)       # img 被切片，降维了。\n",
    "    npimg = img.numpy()         \n",
    "    npimg = np.squeeze( npimg )  # 从1x28x28 变为 28x28， 1维的被 squeeze 删除\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(npimg)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 7])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2102, -0.5019, -0.6006, -0.4131, -0.6475])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f(x) = max(x, 0)\n",
    "# input = torch.randint(2,[2,3,4])\n",
    "input = torch.randn(5)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2102, -0.5019, -0.6006, -0.4131, -0.6475]),\n",
       " tensor([0.2102, 0.0000, 0.0000, 0.0000, 0.0000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, torch.relu(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2102, -0.5019, -0.6006, -0.4131, -0.6475]),\n",
       " LeakyReLU(negative_slope=tensor([ 0.2102, -0.5019, -0.6006, -0.4131, -0.6475])))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input,torch.sigmoid( input )\n",
    "input,torch.tanh( input )\n",
    "input,nn.LeakyReLU( input )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是数据准备和模式测试， \n",
    "以下正式开始设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层感知器模型， 添加1个隐藏层，\n",
    "class Model( nn.Module ):   # 从nn类初始化\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 初始化父类属性\n",
    "        self.linear_1 = nn.Linear(28*28, 120)  # 全连接层、线性层，； 要求输入数据是1维的，所以[1, 28, 28]被压缩到1层28*28；view()\n",
    "        self.linear_2 = nn.Linear(120, 84)  # 线性层， \n",
    "        self.linear_3 = nn.Linear(84, 10)  # 线性层， \n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, 1*28*28)\n",
    "        x = torch.relu(self.linear_1(x))\n",
    "        x = torch.relu(self.linear_2(x))\n",
    "        logits = self.linear_3(x)            # 第3层不做激活函数， 算出在10个分量上，可能值最大的分量，就定义为分量的标签值。即哪个标签分量值最大，就是那个数字。\n",
    "        return logits                         # logits: 一般指未激活前的输出2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- target： 0,1,2,3.。。 并不是一个独热编码的形式？\n",
    "- input： logits\n",
    "- output：？\n",
    "\"\"\"\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "# input = torch.randn(5)\n",
    "# input, np.argmax(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "优化： 根据计算得到的损失，调整模型参数， 降低损失的过程；\n",
    "- Adam 优化器\n",
    "- SGD：优化model的参数、以及lr\n",
    "\"\"\"\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "训练循环：计算准确率correect， 以及每个批次的平均loss\n",
    "\"\"\"\n",
    "def train(dl, model, loss_fn, optimizer):\n",
    "    size = len( dl.dataset )    # 数据多少？\n",
    "    num_batches = len(dl)       # 返回训练批次\n",
    "    train_loss, correct = 0, 0 # 每个批次累计的loss之和， 正确的样本数累计\n",
    "    for x, y in dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)     # 按照损失函数，计算损失\n",
    "        optimizer.zero_grad()       # 清零后计算新的loop的梯度；\n",
    "        loss.backward()             # 按照loss，反向计算梯度；\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            # pred是2维，0维是batch数，1维才是0-9的logits输出；boll转换为float32，累计;item转换到python\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()  \n",
    "            train_loss += loss.item()\n",
    "    correct /= size\n",
    "    train_loss /= num_batches      # losss是以每个批次计算，correct是以个数计算；所以分母不同；\n",
    "    return correct, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数: 计算准确率correect， 以及每个批次的平均loss\n",
    "def test(test_dl, model, loss_fn):\n",
    "    size = len( test_dl.dataset )    # 数据多少？\n",
    "    num_batches = len(test_dl)       # 返回训练批次\n",
    "    test_loss, correct = 0, 0 # 每个批次累计的loss之和， 正确的样本数累计\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dl:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred, y)     # 按照损失函数，计算损失\n",
    "            test_loss += loss.item()\n",
    "            # pred是2维，0维是batch数，1维才是0-9的logits输出；boll转换为float32，累计;item转换到python\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()  \n",
    "        correct /= size\n",
    "        test_loss /= num_batches      # losss是以每个批次计算，correct是以个数计算；所以分母不同；\n",
    "        return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    训练 50 个 epoch， 记录每个epoch的train和test的损失、准确率。 \n",
    "\"\"\"\n",
    "# batch_size 非常关键：4096不准确； 64很准确。16也比较差。 ！！！\n",
    "def fit(epochs, train_dl, test_dl, model, loss_fn, opt ):\n",
    "    # epochs = 20\n",
    "    train_loss, train_acc = [], []\n",
    "    test_loss,  test_acc  = [], []\n",
    "\n",
    "    for epoch in range( epochs ):\n",
    "        epoch_acc, epoch_loss = train( train_dl, model, loss_fn, opt)\n",
    "        epoch_test_acc, epoch_test_loss = test( test_dl, model, loss_fn)\n",
    "        train_acc.append(epoch_acc)\n",
    "        train_loss.append(epoch_loss)\n",
    "        test_acc.append(epoch_test_acc)\n",
    "        test_loss.append(epoch_test_loss)  # 记录、图表化后，观察是否会过拟合等问题\n",
    "        \n",
    "        template = (\"epoch:{:2d}, train_Loss:{:.5f}, train_acc:{:.2f}, test_Loss:{:.5f}, test_acc:{:.2f}, \")\n",
    "        print(template.format( epoch, epoch_loss, epoch_acc*100, epoch_test_loss, epoch_test_acc*100))\n",
    "    print('Done')\n",
    "    return train_loss, train_acc, test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# batch_size = 64   \n",
    "# batch_size = 128\n",
    "batch_size = 1024\n",
    "epochs = 300\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)  # batch_size开始为64\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "train_loss, train_acc, test_loss, test_acc = fit(epochs,train_dl, test_dl, model, loss_fn, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( range(epochs), train_loss, label='train_loss')\n",
    "plt.plot( range(epochs), test_loss, label='test_loss')\n",
    "plt.legend()\n",
    "# train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( range(epochs), train_acc, label='train_acc')\n",
    "plt.plot( range(epochs), test_acc, label='test_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
