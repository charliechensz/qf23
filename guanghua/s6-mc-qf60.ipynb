{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from torch import nn\n",
    "import torchvision  # 图片、视频处理\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F  # 避免relu和sigmoid的初始化，可以直接调用\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split    \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "torch.__version__, torchvision.__version__, device\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math, random\n",
    "# %matplotlib inline\n",
    "# vTest = 1000\n",
    "\n",
    "from importlib import reload \n",
    "from qflib import basic\n",
    "reload(basic)\n",
    "global engine, conn\n",
    "engine = basic.engine()\n",
    "conn = basic.conn(engine)\n",
    "\n",
    "# device = 'cpu'\n",
    "print( device )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToTensor: \n",
    "- 1、输入转为Tensor，\n",
    "- 2、图片格式为 channel, height, width; \n",
    "- 3、像素取值范围规范到0/1\n",
    "\n",
    "torch.utils.data.DataLoader, 作用：\n",
    "- 乱序， shuffle 默认为 True\n",
    "- 将数据采样为小批次， batch_size. batch太小会导致loss的剧烈震荡，太大则内存放不下，也会跨度过大，失去准确性；\n",
    "- num_workers, 子进程设置，更多进程参与\n",
    "- 设置批次处理函数 collate_fn, 用在文本等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165293,\n",
       "    pct_change      diff       dea       bar  jx_days_ud60 jx_xl_250 jx_xl_120  \\\n",
       " 0     -1.1538 -0.108607 -0.190289  0.163366            32      None      None   \n",
       " \n",
       "   jx_xl_60 jx_xl_20 jx_xl_10  ... jx_dg_250  jx_dg_30  jx_dg_20  jx_dg_10  \\\n",
       " 0     None     None     None  ...      None      None   3.38708   9.02793   \n",
       " \n",
       "    jx_dg_5 pct_fl_1_3 pct_fl_3_10 pct_fl_5_20 pct_fl_5_60  pct_fl_20_60  \n",
       " 0  33.1082   -31.6675     104.642     58.8772        None      -20.2301  \n",
       " \n",
       " [1 rows x 42 columns])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据 \n",
    "sql=\"SELECT * FROM ds_qf60\"\n",
    "df = pd.read_sql_query(sql, conn, index_col=None)\n",
    "del df['index']\n",
    "len(df), df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 确定训练的目标 \"\"\"\n",
    "df['class0'] = df['sz_jt_20']  # 10天内的阶梯涨幅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 2.]),\n",
       " 1.0    95821\n",
       " 2.0    46832\n",
       " 0.0    22640\n",
       " Name: class0, dtype: int64,\n",
       " 1.0    0.579704\n",
       " 2.0    0.283327\n",
       " 0.0    0.136969\n",
       " Name: class0, dtype: float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看 class0 是否准确、百分比\n",
    "df.class0.unique(), df.class0.value_counts(), df.class0.value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jx_dg_120</th>\n",
       "      <th>jx_dg_60</th>\n",
       "      <th>jx_dg_20</th>\n",
       "      <th>pct_change</th>\n",
       "      <th>pct_fl_3_10</th>\n",
       "      <th>pct_fl_5_20</th>\n",
       "      <th>pct_fl_20_60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.16319</td>\n",
       "      <td>-15.010000</td>\n",
       "      <td>3.387080</td>\n",
       "      <td>-1.1538</td>\n",
       "      <td>104.64200</td>\n",
       "      <td>58.8772</td>\n",
       "      <td>-20.23010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.00177</td>\n",
       "      <td>-14.688300</td>\n",
       "      <td>9.715350</td>\n",
       "      <td>0.5414</td>\n",
       "      <td>155.15700</td>\n",
       "      <td>57.5850</td>\n",
       "      <td>-21.35470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.34968</td>\n",
       "      <td>-14.228100</td>\n",
       "      <td>4.073800</td>\n",
       "      <td>1.8913</td>\n",
       "      <td>147.90200</td>\n",
       "      <td>37.5448</td>\n",
       "      <td>-24.00660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.72400</td>\n",
       "      <td>6.763570</td>\n",
       "      <td>0.599788</td>\n",
       "      <td>4.3909</td>\n",
       "      <td>116.82900</td>\n",
       "      <td>76.0266</td>\n",
       "      <td>3.86983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.07083</td>\n",
       "      <td>-12.851900</td>\n",
       "      <td>0.507476</td>\n",
       "      <td>0.1154</td>\n",
       "      <td>79.55220</td>\n",
       "      <td>23.4110</td>\n",
       "      <td>25.59640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165288</th>\n",
       "      <td>6.71357</td>\n",
       "      <td>-11.829100</td>\n",
       "      <td>0.265849</td>\n",
       "      <td>-3.6251</td>\n",
       "      <td>214.12400</td>\n",
       "      <td>48.4670</td>\n",
       "      <td>-3.62789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165289</th>\n",
       "      <td>10.40010</td>\n",
       "      <td>-7.082810</td>\n",
       "      <td>7.178250</td>\n",
       "      <td>4.1436</td>\n",
       "      <td>160.64500</td>\n",
       "      <td>39.3249</td>\n",
       "      <td>-9.88584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165290</th>\n",
       "      <td>-19.30980</td>\n",
       "      <td>-0.434874</td>\n",
       "      <td>1.002780</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-21.11130</td>\n",
       "      <td>23.8052</td>\n",
       "      <td>-52.85400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165291</th>\n",
       "      <td>-17.96200</td>\n",
       "      <td>1.521940</td>\n",
       "      <td>6.686400</td>\n",
       "      <td>0.9281</td>\n",
       "      <td>-5.23023</td>\n",
       "      <td>22.4991</td>\n",
       "      <td>-53.25980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165292</th>\n",
       "      <td>-18.36630</td>\n",
       "      <td>2.718480</td>\n",
       "      <td>8.702470</td>\n",
       "      <td>0.4662</td>\n",
       "      <td>33.53840</td>\n",
       "      <td>36.1571</td>\n",
       "      <td>-52.81490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165293 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        jx_dg_120   jx_dg_60  jx_dg_20  pct_change  pct_fl_3_10  pct_fl_5_20  \\\n",
       "0         8.16319 -15.010000  3.387080     -1.1538    104.64200      58.8772   \n",
       "1         7.00177 -14.688300  9.715350      0.5414    155.15700      57.5850   \n",
       "2         6.34968 -14.228100  4.073800      1.8913    147.90200      37.5448   \n",
       "3       -10.72400   6.763570  0.599788      4.3909    116.82900      76.0266   \n",
       "4        -1.07083 -12.851900  0.507476      0.1154     79.55220      23.4110   \n",
       "...           ...        ...       ...         ...          ...          ...   \n",
       "165288    6.71357 -11.829100  0.265849     -3.6251    214.12400      48.4670   \n",
       "165289   10.40010  -7.082810  7.178250      4.1436    160.64500      39.3249   \n",
       "165290  -19.30980  -0.434874  1.002780      0.0000    -21.11130      23.8052   \n",
       "165291  -17.96200   1.521940  6.686400      0.9281     -5.23023      22.4991   \n",
       "165292  -18.36630   2.718480  8.702470      0.4662     33.53840      36.1571   \n",
       "\n",
       "        pct_fl_20_60  \n",
       "0          -20.23010  \n",
       "1          -21.35470  \n",
       "2          -24.00660  \n",
       "3            3.86983  \n",
       "4           25.59640  \n",
       "...              ...  \n",
       "165288      -3.62789  \n",
       "165289      -9.88584  \n",
       "165290     -52.85400  \n",
       "165291     -53.25980  \n",
       "165292     -52.81490  \n",
       "\n",
       "[165293 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  'diff', 'dea',, \n",
    "# X_columns0 = ['pct_change', \n",
    "#              'jx_days_ud60', \n",
    "#              'jx_xl_120','jx_xl_60', 'jx_xl_20', \n",
    "#              'jx_zs_20', \n",
    "#              'lj_fl_5_20','lj_fl_20_60']\n",
    "X_columns = ['jx_dg_120','jx_dg_60', 'jx_dg_20', \n",
    "             'pct_change', \n",
    "             'pct_fl_3_10','pct_fl_5_20','pct_fl_20_60']\n",
    "df[ X_columns ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([165293, 7]), torch.Size([165293]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = df[X_columns]  \n",
    "X = torch.from_numpy(X_data.values).type(torch.float32)\n",
    "# Y_data = df.class0.values.reshape(-1,0)  # 转换成 pd type array\n",
    "Y_data = df.class0.values  # 转换成 pd type array\n",
    "Y = torch.from_numpy(Y_data).long()\n",
    "# Y = torch.from_numpy(Y_data).type(torch.float32)\n",
    "# X_data.shape, X_data.head(3)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 2.]), array([22640, 95821, 46832], dtype=int64))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最后检测Y_data数据准确性\n",
    "unique, counts = np.unique(Y_data, return_counts=True)\n",
    "unique, counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X,Y 形成main dataset, train/test dataset \n",
    "X = X.to(device)\n",
    "Y = Y.to(device)\n",
    "main_dataset = TensorDataset(X, Y)\n",
    "train_ds, test_ds = random_split(main_dataset, [0.8, 0.2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_ds = torchvision.datasets.MNIST('data', \n",
    "#                                       train=True, \n",
    "#                                       download=True, \n",
    "#                                       transform=ToTensor())\n",
    "# test_ds = torchvision.datasets.MNIST('data', \n",
    "#                                      train=False , \n",
    "#                                      download=True, \n",
    "#                                      transform=ToTensor())\n",
    "# # test_ds = test_ds.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4\n",
    "batch_size = 16\n",
    "# batch_size = 64\n",
    "# batch_size = 4096\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)  # batch_size开始为64,可能是最佳，或128\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 7]), torch.Size([16]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, labels = next(iter(train_dl))   # train dataloder 可以分解出X和Ydata\n",
    "imgs.shape, labels.shape   # 1: 黑白图片， 28x28的图片分辨率， 64张\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs[0].shape, imgs[0, 0, 0]\n",
    "# imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各种激活函数测试\n",
    "# f(x) = max(x, 0)\n",
    "# input = torch.randint(2,[2,3,4])\n",
    "# input = torch.randn(5)\n",
    "# input\n",
    "# input, torch.relu(input)\n",
    "\n",
    "# input,torch.sigmoid( input )\n",
    "# input,torch.tanh( input )\n",
    "# input,nn.LeakyReLU( input )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是数据准备和模式测试， \n",
    "以下正式开始设计："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层感知器模型， 添加1个隐藏层，\n",
    "class Model3( nn.Module ):   # 从nn类初始化\n",
    "    def __init__(self):\n",
    "        super().__init__()  # 初始化父类属性\n",
    "        self.linear_1 = nn.Linear(7, 120)  # 全连接层、线性层，； 要求输入数据是1维的，所以[1, 28, 28]被压缩到1层28*28；view()\n",
    "        self.linear_2 = nn.Linear(120, 84)  # 线性层， \n",
    "        self.linear_3 = nn.Linear(84, 3)  # 线性层， \n",
    "    def forward(self, input):\n",
    "        x = input.view(-1, 7)\n",
    "        x = torch.relu(self.linear_1(x))\n",
    "        x = torch.relu(self.linear_2(x))\n",
    "        logits = self.linear_3(x)            # 第3层不做激活函数， 算出在10个分量上，可能值最大的分量，就定义为分量的标签值。即哪个标签分量值最大，就是那个数字。\n",
    "        return logits                         # logits: 一般指未激活前的输出2\n",
    "\n",
    "# class Model( nn.Module ):   # 从nn类初始化\n",
    "#     def __init__(self):\n",
    "#         super().__init__()  # 初始化父类属性\n",
    "#         self.linear_1 = nn.Linear(7, 64)  # 全连接层、线性层，； 要求输入数据是1维的，所以[1, 28, 28]被压缩到1层28*28；view()\n",
    "#         self.linear_2 = nn.Linear(64, 64)  # 线性层， \n",
    "#         self.linear_3 = nn.Linear(64, 32)  # 线性层， \n",
    "#         self.linear_4 = nn.Linear(32, 4)  # 线性层， \n",
    "#     def forward(self, input):\n",
    "#         x = input.view(-1, 7)\n",
    "#         x = torch.relu(self.linear_1(x))\n",
    "#         x = torch.relu(self.linear_2(x))\n",
    "#         x = torch.relu(self.linear_3(x))\n",
    "#         logits = self.linear_4(x)            # 第3层不做激活函数， 算出在10个分量上，可能值最大的分量，就定义为分量的标签值。即哪个标签分量值最大，就是那个数字。\n",
    "#         return logits                         # logits: 一般指未激活前的输出2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- target： 0,1,2,3.。。 并不是一个独热编码的形式？\n",
    "- input： logits\n",
    "- output：？\n",
    "\"\"\"\n",
    "loss_fn = nn.CrossEntropyLoss()  \n",
    "# loss_fn = nn.NLLLoss()  \n",
    "# input = torch.randn(5)\n",
    "# input, np.argmax(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "优化： 根据计算得到的损失，调整模型参数， 降低损失的过程；\n",
    "- Adam 优化器\n",
    "- SGD：优化model的参数、以及lr\n",
    "\"\"\"\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "训练循环：计算准确率correect， 以及每个批次的平均loss\n",
    "\"\"\"\n",
    "def train(dl, model, loss_fn, optimizer):\n",
    "    size = len( dl.dataset )    # 数据多少？\n",
    "    num_batches = len(dl)       # 返回训练批次\n",
    "    train_loss, correct = 0, 0 # 每个批次累计的loss之和， 正确的样本数累计\n",
    "    for x, y in dl:\n",
    "        # x, y = x.to(device), y.to(device)\n",
    "        pred = model(x)\n",
    "        # pred = pred.to(torch.FloatTensor)\n",
    "        loss = loss_fn(pred, y)     # 按照损失函数，计算损失\n",
    "        optimizer.zero_grad()       # 清零后计算新的loop的梯度；\n",
    "        loss.backward()             # 按照loss，反向计算梯度；\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            # pred是2维，0维是batch数，1维才是0-9的logits输出；boll转换为float32，累计;item转换到python\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()  \n",
    "            train_loss += loss.item()\n",
    "    correct /= size\n",
    "    train_loss /= num_batches      # losss是以每个批次计算，correct是以个数计算；所以分母不同；\n",
    "    return correct, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数: 计算准确率correect， 以及每个批次的平均loss\n",
    "def test(test_dl, model, loss_fn):\n",
    "    size = len( test_dl.dataset )    # 数据多少？\n",
    "    num_batches = len(test_dl)       # 返回训练批次\n",
    "    test_loss, correct = 0, 0 # 每个批次累计的loss之和， 正确的样本数累计\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_dl:\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            # pred = pred.to(torch.FloatTensor)\n",
    "            loss = loss_fn(pred, y)     # 按照损失函数，计算损失\n",
    "            test_loss += loss.item()\n",
    "            # pred是2维，0维是batch数，1维才是0-9的logits输出；boll转换为float32，累计;item转换到python\n",
    "            correct += (pred.argmax(1)==y).type(torch.float).sum().item()  \n",
    "        correct /= size\n",
    "        test_loss /= num_batches      # losss是以每个批次计算，correct是以个数计算；所以分母不同；\n",
    "        return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    训练 N 个 epoch， 记录每个epoch的train和test的损失、准确率。 \n",
    "\"\"\"\n",
    "# batch_size 非常关键：4096不准确； 64很准确。16也比较差。 ！！！\n",
    "def fit(epochs, train_dl, test_dl, model, loss_fn, opt ):\n",
    "    # epochs = 20\n",
    "    train_loss, train_acc = [], []\n",
    "    test_loss,  test_acc  = [], []\n",
    "\n",
    "    for epoch in range( epochs ):\n",
    "        epoch_acc, epoch_loss = train( train_dl, model, loss_fn, opt)\n",
    "        epoch_test_acc, epoch_test_loss = test( test_dl, model, loss_fn)\n",
    "        train_acc.append(epoch_acc)\n",
    "        train_loss.append(epoch_loss)\n",
    "        test_acc.append(epoch_test_acc)\n",
    "        test_loss.append(epoch_test_loss)  # 记录、图表化后，观察是否会过拟合等问题\n",
    "        \n",
    "        template = (\"epoch:{:2d}, train_Loss:{:.5f}, train_acc:{:.2f}, test_Loss:{:.5f}, test_acc:{:.2f}, \")\n",
    "        print(template.format( epoch, epoch_loss, epoch_acc*100, epoch_test_loss, epoch_test_acc*100))\n",
    "    print('Done')\n",
    "    return train_loss, train_acc, test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_Loss:0.96394, train_acc:57.77, test_Loss:0.95228, test_acc:57.77, \n",
      "epoch: 1, train_Loss:0.94823, train_acc:58.19, test_Loss:0.94753, test_acc:57.86, \n",
      "epoch: 2, train_Loss:0.94339, train_acc:58.27, test_Loss:0.94543, test_acc:57.85, \n",
      "epoch: 3, train_Loss:0.94017, train_acc:58.29, test_Loss:0.94745, test_acc:57.91, \n",
      "epoch: 4, train_Loss:0.93833, train_acc:58.30, test_Loss:0.94483, test_acc:57.74, \n",
      "epoch: 5, train_Loss:0.93681, train_acc:58.28, test_Loss:0.94436, test_acc:57.74, \n",
      "epoch: 6, train_Loss:0.93550, train_acc:58.37, test_Loss:0.93947, test_acc:57.90, \n",
      "epoch: 7, train_Loss:0.93460, train_acc:58.36, test_Loss:0.94351, test_acc:57.89, \n",
      "epoch: 8, train_Loss:0.93408, train_acc:58.38, test_Loss:0.94062, test_acc:58.01, \n",
      "epoch: 9, train_Loss:0.93321, train_acc:58.39, test_Loss:0.93916, test_acc:57.91, \n",
      "epoch:10, train_Loss:0.93285, train_acc:58.38, test_Loss:0.93797, test_acc:57.96, \n",
      "epoch:11, train_Loss:0.93243, train_acc:58.42, test_Loss:0.93858, test_acc:57.86, \n",
      "epoch:12, train_Loss:0.93199, train_acc:58.44, test_Loss:0.93846, test_acc:57.89, \n",
      "epoch:13, train_Loss:0.93171, train_acc:58.42, test_Loss:0.93985, test_acc:58.04, \n",
      "epoch:14, train_Loss:0.93128, train_acc:58.45, test_Loss:0.93672, test_acc:57.98, \n",
      "epoch:15, train_Loss:0.93107, train_acc:58.50, test_Loss:0.93791, test_acc:58.02, \n",
      "epoch:16, train_Loss:0.93079, train_acc:58.48, test_Loss:0.93703, test_acc:58.10, \n",
      "epoch:17, train_Loss:0.93060, train_acc:58.49, test_Loss:0.93693, test_acc:58.01, \n",
      "epoch:18, train_Loss:0.93012, train_acc:58.50, test_Loss:0.93556, test_acc:58.00, \n",
      "epoch:19, train_Loss:0.92986, train_acc:58.48, test_Loss:0.93766, test_acc:58.06, \n",
      "epoch:20, train_Loss:0.92972, train_acc:58.52, test_Loss:0.93700, test_acc:57.98, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m# train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)  # batch_size开始为64\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m# test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_loss, train_acc, test_loss, test_acc \u001b[39m=\u001b[39m fit(epochs,train_dl, test_dl, model, loss_fn, opt)\n",
      "Cell \u001b[1;32mIn[59], line 11\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(epochs, train_dl, test_dl, model, loss_fn, opt)\u001b[0m\n\u001b[0;32m      8\u001b[0m test_loss,  test_acc  \u001b[39m=\u001b[39m [], []\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m( epochs ):\n\u001b[1;32m---> 11\u001b[0m     epoch_acc, epoch_loss \u001b[39m=\u001b[39m train( train_dl, model, loss_fn, opt)\n\u001b[0;32m     12\u001b[0m     epoch_test_acc, epoch_test_loss \u001b[39m=\u001b[39m test( test_dl, model, loss_fn)\n\u001b[0;32m     13\u001b[0m     train_acc\u001b[39m.\u001b[39mappend(epoch_acc)\n",
      "Cell \u001b[1;32mIn[56], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dl, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()       \u001b[39m# 清零后计算新的loop的梯度；\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()             \u001b[39m# 按照loss，反向计算梯度；\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m     17\u001b[0m     \u001b[39m# pred是2维，0维是batch数，1维才是0-9的logits输出；boll转换为float32，累计;item转换到python\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (pred\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)\u001b[39m==\u001b[39my)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()  \n",
      "File \u001b[1;32mc:\\Users\\charlie\\miniconda3\\envs\\cuda\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\charlie\\miniconda3\\envs\\cuda\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\charlie\\miniconda3\\envs\\cuda\\lib\\site-packages\\torch\\optim\\sgd.py:76\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     72\u001b[0m momentum_buffer_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     74\u001b[0m has_sparse_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(group, params_with_grad, d_p_list, momentum_buffer_list)\n\u001b[1;32m---> 76\u001b[0m sgd(params_with_grad,\n\u001b[0;32m     77\u001b[0m     d_p_list,\n\u001b[0;32m     78\u001b[0m     momentum_buffer_list,\n\u001b[0;32m     79\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     80\u001b[0m     momentum\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmomentum\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     81\u001b[0m     lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     82\u001b[0m     dampening\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdampening\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     83\u001b[0m     nesterov\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mnesterov\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     84\u001b[0m     maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     85\u001b[0m     has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m     86\u001b[0m     foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     88\u001b[0m \u001b[39m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39mfor\u001b[39;00m p, momentum_buffer \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\charlie\\miniconda3\\envs\\cuda\\lib\\site-packages\\torch\\optim\\sgd.py:222\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 222\u001b[0m func(params,\n\u001b[0;32m    223\u001b[0m      d_p_list,\n\u001b[0;32m    224\u001b[0m      momentum_buffer_list,\n\u001b[0;32m    225\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    226\u001b[0m      momentum\u001b[39m=\u001b[39;49mmomentum,\n\u001b[0;32m    227\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    228\u001b[0m      dampening\u001b[39m=\u001b[39;49mdampening,\n\u001b[0;32m    229\u001b[0m      nesterov\u001b[39m=\u001b[39;49mnesterov,\n\u001b[0;32m    230\u001b[0m      has_sparse_grad\u001b[39m=\u001b[39;49mhas_sparse_grad,\n\u001b[0;32m    231\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize)\n",
      "File \u001b[1;32mc:\\Users\\charlie\\miniconda3\\envs\\cuda\\lib\\site-packages\\torch\\optim\\sgd.py:325\u001b[0m, in \u001b[0;36m_multi_tensor_sgd\u001b[1;34m(params, grads, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    322\u001b[0m         device_grads \u001b[39m=\u001b[39m bufs\n\u001b[0;32m    324\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m device_has_sparse_grad:\n\u001b[1;32m--> 325\u001b[0m     torch\u001b[39m.\u001b[39;49m_foreach_add_(device_params, device_grads, alpha\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mlr)\n\u001b[0;32m    326\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     \u001b[39m# foreach APIs don't support sparse\u001b[39;00m\n\u001b[0;32m    328\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(device_params)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batch_size = 16\n",
    "# batch_size = 64   \n",
    "# batch_size = 128\n",
    "# batch_size = 4\n",
    "epochs = 100\n",
    "# train_dl = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)  # batch_size开始为64\n",
    "# test_dl = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "train_loss, train_acc, test_loss, test_acc = fit(epochs,train_dl, test_dl, model, loss_fn, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( range(epochs), train_loss, label='train_loss')\n",
    "plt.plot( range(epochs), test_loss, label='test_loss')\n",
    "plt.legend()\n",
    "# train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( range(epochs), train_acc, label='train_acc')\n",
    "plt.plot( range(epochs), test_acc, label='test_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
